# -*- coding: utf-8 -*-
"""K projects.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YNlShbyD9yapLFR3AE4clQPRNGE-RlRg

# **Customer Churn Prediction Model - Telco Customer Churn**

##  Context

In today's highly competitive market, customer retention is just as important—if not more so—than customer acquisition. This is especially true in industries like telecom, where customer acquisition costs are high and profit margins depend on long-term customer relationships.

Telecommunication companies often face a major challenge: customers cancel their subscriptions and switch to competitors without notice. This phenomenon, known as **customer churn**, directly impacts the company's revenue, growth, and market share. Retaining existing customers is more cost-effective than acquiring new ones, making churn prediction a top priority for telecom businesses.

## Problem Statement
Customer churn poses a major challenge for telecom companies, impacting both revenue and long-term growth. Accurately predicting which customers are likely to leave is crucial for implementing effective retention strategies. However, due to factors such as class imbalance and varied customer behavior, predicting churn remains a complex problem that requires advanced data analysis and machine learning techniques.

This project aims to develop an accurate customer churn prediction model using machine learning. The dataset includes customer demographic details, service usage patterns, and subscription information. The goal is to identify the most relevant features, preprocess the data effectively, and train multiple classifiers to predict churn with high accuracy.

## Objective


This project involves building a **machine learning-based customer churn prediction system** using a telecom customer dataset. By analyzing this data, we aim to understand patterns associated with customer churn and build a predictive model that helps businesses identify at-risk customers in advance.

In a real-world scenario, company executives and marketing teams need reliable tools to proactively identify customers at risk of leaving. With accurate churn prediction models, businesses can:
- Offer targeted retention campaigns or discounts.
- Improve customer satisfaction by addressing pain points early.
- Optimize resource allocation for customer support and outreach.

This project simulates such a real-world challenge by using historical telecom customer data to build a machine learning model that predicts churn. The insights gained from this analysis can directly support business strategies aimed at improving customer loyalty and reducing revenue loss due to churn.

##  Data Dictionary – Telecom Customer Churn Dataset

| Column Name        | Description                                                                 | Data Type        |
|--------------------|-----------------------------------------------------------------------------|------------------|
| `customerID`       | Unique identifier for each customer                                         | Categorical (ID) |
| `gender`           | Gender of the customer (`Male`, `Female`)                                  | Categorical      |
| `SeniorCitizen`    | Indicates if the customer is a senior citizen (`0`: No, `1`: Yes)           | Numerical (binary) |
| `Partner`          | Whether the customer has a partner (`Yes` / `No`)                           | Categorical      |
| `Dependents`       | Whether the customer has dependents (`Yes` / `No`)                          | Categorical      |
| `tenure`           | Number of months the customer has stayed with the company                   | Numerical        |
| `PhoneService`     | Whether the customer has a phone service                                    | Categorical      |
| `MultipleLines`    | Indicates if the customer has multiple phone lines                          | Categorical      |
| `InternetService`  | Type of internet service: `DSL`, `Fiber optic`, or `No`                     | Categorical      |
| `OnlineSecurity`   | Whether the customer has online security service                            | Categorical      |
| `OnlineBackup`     | Whether the customer has online backup service                              | Categorical      |
| `DeviceProtection` | Whether the customer has device protection                                  | Categorical      |
| `TechSupport`      | Whether the customer has tech support                                       | Categorical      |
| `StreamingTV`      | Whether the customer streams TV services                                    | Categorical      |
| `StreamingMovies`  | Whether the customer streams movies                                         | Categorical      |
| `Contract`         | Type of contract: `Month-to-month`, `One year`, or `Two year`               | Categorical      |
| `PaperlessBilling` | Whether the customer uses paperless billing                                 | Categorical      |
| `PaymentMethod`    | Payment method: e.g., `Electronic check`, `Mailed check`, etc.              | Categorical      |
| `MonthlyCharges`   | The amount charged to the customer monthly                                  | Numerical        |
| `TotalCharges`     | Total amount charged to the customer to date                                | Numerical        |
| `Churn`            | Target variable – whether the customer has churned (`Yes` / `No`)           | Categorical      |

```

#1. Importing the dependencies
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import pickle

"""#2. Data Loading and Understanding"""

# load teh csv data to a pandas dataframe
df = pd.read_csv("/content/WA_Fn-UseC_-Telco-Customer-Churn.csv")

df.shape

df.head()

df.info()

# dropping customerID column as this is not required for modelling
df = df.drop(columns=["customerID"])

df.columns

print(df["gender"].unique())

print(df["SeniorCitizen"].unique())

# printing the unique values in all the columns

numerical_features_list = ["tenure", "MonthlyCharges", "TotalCharges"]

for col in df.columns:
  if col not in numerical_features_list:
    print(col, df[col].unique())
    print("-"*50)

print(df.isnull().sum())

#df["TotalCharges"] = df["TotalCharges"].astype(float)

df[df["TotalCharges"]==" "]

len(df[df["TotalCharges"]==" "])

df["TotalCharges"] = df["TotalCharges"].replace({" ": "0.0"})

df["TotalCharges"] = df["TotalCharges"].astype(float)

df.info()

# checking the class distribution of target column
print(df["Churn"].value_counts())

"""### Data Insights

- **Customer ID Removed**: The `CustomerID` column was dropped as it is not relevant for modeling purposes.  
- **No Missing Values**: The dataset does not contain any missing values in most columns.  
- **TotalCharges Column Cleaned**: Missing values in the `TotalCharges` column were identified and replaced with 0.  
- **Class Imbalance Detected**: The target variable shows an imbalance in class distribution, which may affect model performance.

#3. Exploratory Data Analysis (EDA)
"""

df.describe()

"""##Numerical Features - Analysis"""

def plot_histogram(df, column_name):

  plt.figure(figsize=(5, 3))
  sns.histplot(df[column_name], kde=True)
  plt.title(f"Distribution of {column_name}")

  # calculate the mean and median values for the columns
  col_mean = df[column_name].mean()
  col_median = df[column_name].median()

  # add vertical lines for mean and median
  plt.axvline(col_mean, color="red", linestyle="--", label="Mean")
  plt.axvline(col_median, color="green", linestyle="-", label="Median")

  plt.legend()

  plt.show()

plot_histogram(df, "tenure")

plot_histogram(df, "MonthlyCharges")

plot_histogram(df, "TotalCharges")

##Box plot for numerical features
def plot_boxplot(df, column_name):

  plt.figure(figsize=(5, 3))
  sns.boxplot(y=df[column_name])
  plt.title(f"Box Plot of {column_name}")
  plt.ylabel(column_name)
  plt.show

plot_boxplot(df, "tenure")

plot_boxplot(df, "MonthlyCharges")

plot_boxplot(df, "TotalCharges")

"""
###Correlation Heatmap for numerical columns"""

plt.figure(figsize=(8, 4))
sns.heatmap(df[["tenure", "MonthlyCharges", "TotalCharges"]].corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

"""## Observations and Insights

➤ Dataset Overview

- Rows/Columns: The dataset contains 7043 records and 21 features (after dropping customerID).

- Target Variable: Churn is binary (Yes/No) indicating if a customer has left the service.

➤  Numerical Features Analysis

1.  Tenure (Customer duration in months) :
- Distribution: Right-skewed; many customers have short tenure.

- Insight: A high number of customers leave within the first 10 months, indicating possible dissatisfaction in early service stages.

- Business Action: Improve onboarding experience and early customer support.

2.  MonthlyCharges:

- Distribution: Nearly uniform with a slight right skew.

- Insight: No sharp pattern of churn based on monthly charges alone, but high charges might correlate with add-on services (e.g., streaming).

- Outliers: Some high values; could indicate customers with premium services.

3.  TotalCharges :

- Initial Issue: Contained missing/empty strings—cleaned by converting to 0.0 and float.

- Distribution: Heavily right-skewed.

- Insight: New customers (low tenure) have low total charges, as expected. This feature correlates strongly with tenure (≈0.83).

- Boxplot Insight: Presence of outliers — check for churn risk in high spenders

----- Insight: Tenure is the biggest contributor to TotalCharges, validating its redundancy.

##Categorical features - Analysis
"""

df.columns

"""##Countplot for categorical columns"""

object_cols = df.select_dtypes(include="object").columns.to_list()

object_cols = ["SeniorCitizen"] + object_cols

for col in object_cols:
  plt.figure(figsize=(5, 3))
  sns.countplot(x=df[col])
  plt.title(f"Count Plot of {col}")
  plt.show()

"""3. Categorical Feature Insights

➤ gender
- Roughly equal split between Male and Female.

- No significant impact on churn.

➤ SeniorCitizen
- Only ~16% are senior citizens.

- Churn is slightly higher among seniors.

- Recommendation: Offer senior-specific packages or support.

➤ Partner & Dependents
- Customers without partners or dependents are more likely to churn.

- Business Insight: These may represent younger or single users who are less loyal or price-sensitive.

➤ InternetService
- Fiber optic users have a higher churn rate.

- DSL users churn less.

- No internet users churn the least, likely because they are low-engagement customers.

- Suggestion: Investigate dissatisfaction with fiber service.

➤ OnlineSecurity, TechSupport, DeviceProtection, OnlineBackup
- Churn is significantly higher among those who don’t use these services.

- Insight: These are strong indicators of customer engagement and satisfaction.

- EngagementScore Feature: Smart inclusion to quantify overall digital engagement.

➤ Contract
- Most important churn indicator.

- Month-to-month contract holders churn far more than those on 1 or 2-year contracts.

- Insight: Longer-term contracts promote retention.

- Business Action: Incentivize contract upgrades.

➤ PaperlessBilling
- Users with paperless billing churn more.

- Possibly reflects digital-first or younger users, or billing confusion.

➤ PaymentMethod
- Electronic check users churn much more than others (like credit cards or bank transfers).

- Might indicate billing issues or low trust.

- Suggestion: Investigate issues around this method or promote alternative billing options.

##  Data Preprocessing
"""

df.head(5)

"""## Feature Engineeering"""

df["AvgMonthlySpend"] = df["TotalCharges"] / (df["tenure"] + 1)

df["HasStreaming"] = ((df["StreamingTV"] == "Yes") | (df["StreamingMovies"] == "Yes")).astype(int)

df["EngagementScore"] = (
    (df["OnlineSecurity"] == "Yes").astype(int) +
    (df["OnlineBackup"] == "Yes").astype(int) +
    (df["DeviceProtection"] == "Yes").astype(int) +
    (df["TechSupport"] == "Yes").astype(int) +
    df["HasStreaming"]
)

df["IsLongTermContract"] = df["Contract"].apply(lambda x: 0 if x == "Month-to-month" else 1)

"""## Label encoding of target column


"""

print(df["Churn"].value_counts())

"""## Label encoding of categorical fetaures"""

# identifying columns with object data type
object_columns = df.select_dtypes(include="object").columns

print(object_columns)

# initializing a dictionary to save the encoders
encoders = {}

# applying label encoding and store the encoders
for column in object_columns:
  label_encoder = LabelEncoder()
  df[column] = label_encoder.fit_transform(df[column])
  encoders[column] = label_encoder


# saving the encoders to a pickle file
with open("encoders.pkl", "wb") as f:
  pickle.dump(encoders, f)

encoders

df.head()

"""## Training and test data split"""

# === Feature Engineering ===
df["AvgMonthlySpend"] = df["TotalCharges"] / (df["tenure"] + 1)
df["HasStreaming"] = ((df["StreamingTV"] == 1) | (df["StreamingMovies"] == 1)).astype(int)
df["IsEngaged"] = df[["OnlineSecurity", "OnlineBackup", "StreamingTV", "StreamingMovies", "TechSupport"]].sum(axis=1)
df["IsLongTermContract"] = df["Contract"].apply(lambda x: 0 if x == 0 else 1)  # Assuming 0 is Month-to-month

# splitting the features and target
X = df.drop(columns=["Churn"])
y = df["Churn"]

# split training and test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(y_train.shape)

print(y_train.value_counts())

"""##Synthetic Minority Oversampling Technique (SMOTE)"""

from imblearn.combine import SMOTETomek

smt = SMOTETomek(random_state=42)
X_train_resampled, y_train_resampled = smt.fit_resample(X_train, y_train)

print("Resampled dataset shape:", y_train_resampled.value_counts())

"""
## Model Training"""

# dictionary of models
models = {
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "XGBoost": XGBClassifier(random_state=42)
}

# dictionary to store the cross validation results
cv_scores = {}

# performing the 5-fold cross validation for each model
for model_name, model in models.items():
  print(f"Training {model_name} with default parameters")
  scores = cross_val_score(model, X_train_resampled, y_train_resampled, cv=5, scoring="accuracy")
  cv_scores[model_name] = scores
  print(f"{model_name} cross-validation accuracy: {np.mean(scores):.2f}")
  print("-"*70)

cv_scores

"""- **Model Performance**: Among all the models tested with default parameters, **Random Forest** achieved the highest accuracy.

## Hyperparameter Tuning
"""

from sklearn.model_selection import GridSearchCV
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [5, 10],
    'min_samples_split': [2, 5]
}


grid_search = GridSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42),
                           param_grid, cv=5, scoring='accuracy', n_jobs=-1)

grid_search.fit(X_train_resampled, y_train_resampled)

best_model = grid_search.best_estimator_
print("Best Parameters:", grid_search.best_params_)

from sklearn.model_selection import RandomizedSearchCV

random_search = RandomizedSearchCV(model, param_distributions=param_grid,
                                   n_iter=10, scoring='accuracy', cv=5, n_jobs=-1, random_state=42)
random_search.fit(X_train_resampled, y_train_resampled)

rfc = RandomForestClassifier(class_weight='balanced', random_state=42)

rfc.fit( X_train_resampled, y_train_resampled)

print(y_test.value_counts())

"""##  Model Evaluation

"""

rfc = RandomForestClassifier(n_estimators=100, random_state=42)
rfc.fit(X_train_resampled, y_train_resampled)  # Make sure you train it
y_test_pred = rfc.predict(X_test)

# evaluate on test data
y_test_pred = rfc.predict(X_test)

print("Accuracy Score:\n", accuracy_score(y_test, y_test_pred))
print("Confsuion Matrix:\n", confusion_matrix(y_test, y_test_pred))
print("Classification Report:\n", classification_report(y_test, y_test_pred))

"""## ROC-AUC and Precision-Recall AUC"""

from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, auc

y_proba = rfc.predict_proba(X_test)[:, 1]

# ROC-AUC
roc_auc = roc_auc_score(y_test, y_proba)
print("ROC-AUC Score:", roc_auc)

fpr, tpr, _ = roc_curve(y_test, y_proba)
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()

# Precision-Recall AUC
precision, recall, _ = precision_recall_curve(y_test, y_proba)
pr_auc = auc(recall, precision)
print("PR-AUC Score:", pr_auc)

plt.figure(figsize=(6, 4))
plt.plot(recall, precision, label=f"PR Curve (AUC = {pr_auc:.2f})")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend()
plt.show()

# Confusionm Matrix
cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
report = classification_report(y_test, y_test_pred, output_dict=True)
report_df = pd.DataFrame(report).iloc[:-1, :].T  # exclude 'accuracy'
plt.figure(figsize=(8, 4))
sns.heatmap(report_df, annot=True, cmap='YlGnBu', fmt=".2f")
plt.title("Classification Report")
plt.show()

from sklearn.metrics import accuracy_score

# Predict on the test set
y_pred = best_model.predict(X_test)

# Calculate accuracy
final_accuracy = accuracy_score(y_test, y_pred)

# Print the final accuracy
print(f"Final Accuracy on Test Set: {final_accuracy:.4f}")

# Final predictions
y_pred = rfc.predict(X_test)

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"\n Accuracy Score: {accuracy:.4f}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()
print(f"\n Confusion Matrix:\n{cm}")
print(f"  True Negatives (Non-Churn correctly predicted): {tn}")
print(f"  False Positives (Non-Churn predicted as Churn): {fp}")
print(f"  False Negatives (Churn predicted as Non-Churn): {fn}")
print(f"  True Positives (Churn correctly predicted): {tp}")

# Classification Report
report = classification_report(y_test, y_pred, output_dict=True)
print("\n Classification Report:")
print(f"  Class 0 - Non-Churn → Precision: {report['0']['precision']:.2f}, Recall: {report['0']['recall']:.2f}, F1: {report['0']['f1-score']:.2f}")
print(f"  Class 1 - Churn     → Precision: {report['1']['precision']:.2f}, Recall: {report['1']['recall']:.2f}, F1: {report['1']['f1-score']:.2f}")
print(f"\n  Macro Avg           → F1 Score: {report['macro avg']['f1-score']:.2f}")
print(f"  Weighted Avg        → F1 Score: {report['weighted avg']['f1-score']:.2f}")

"""### Insights & Observations

- The model achieves a solid accuracy of 77.79%.

- While accuracy is a useful general measure, it's not the most important metric in churn problems due to class imbalance.

- False Negatives (139): These are churners the model failed to identify — a critical area of improvement, as these are customers at risk who may leave without any intervention.

#  Improving Recall

The Need to Improve Recall for Churn :  
- Business Goal: In churn prediction, recall for churners (Class 1) is more important than precision because:

- Missing a churner = potential lost revenue.

- Reducing false negatives would help recover more at-risk customers and increase retention ROI.

## Optimize Decision Threshold
"""

from sklearn.metrics import confusion_matrix, classification_report

# Predict probabilities
y_probs = rfc.predict_proba(X_test)[:, 1]

# Tune threshold
threshold = 0.35  # Lower threshold to favor recall
y_pred_thresh = (y_probs >= threshold).astype(int)

# New Evaluation
print("Confusion Matrix with Threshold = 0.35:")
print(confusion_matrix(y_test, y_pred_thresh))

print("Classification Report with Threshold = 0.35:")
print(classification_report(y_test, y_pred_thresh))

"""## Ploting Recall vs Threshold Curve

- To choose the best threshold visually:
"""

from sklearn.metrics import recall_score

thresholds = np.arange(0.1, 0.9, 0.05)
recalls = [recall_score(y_test, y_probs > t) for t in thresholds]

plt.figure(figsize=(6, 4))
plt.plot(thresholds, recalls, marker='o')
plt.title("Recall vs Threshold")
plt.xlabel("Threshold")
plt.ylabel("Recall")
plt.grid(True)
plt.show()

"""### Use Class Weights (Cost-sensitive Learning)

- Updating the Random Forest to penalize misclassifying churners more:
"""

rfc_weighted = RandomForestClassifier(n_estimators=100, class_weight={0: 1, 1: 3}, random_state=42)
rfc_weighted.fit(X_train_resampled, y_train_resampled)

# Predict and evaluate
y_pred_weighted = rfc_weighted.predict(X_test)
print("Classification Report with Class Weights:")
print(classification_report(y_test, y_pred_weighted))

"""### Trying Ensemble of Models (Soft Voting Classifier)

- Combining strengths of multiple models:
"""

from sklearn.ensemble import VotingClassifier

clf1 = RandomForestClassifier(class_weight='balanced', n_estimators=100, random_state=42)
clf2 = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

ensemble_model = VotingClassifier(
    estimators=[('rf', clf1), ('xgb', clf2)],
    voting='soft'  # use probabilities
)

ensemble_model.fit(X_train_resampled, y_train_resampled)
y_ensemble_pred = ensemble_model.predict(X_test)

print("Ensemble Model Classification Report:")
print(classification_report(y_test, y_ensemble_pred))

"""#### Evaluating False Negatives (Missed Churners)

- After making predictions:
"""

# Identify false negatives
false_negatives = (y_test == 1) & (y_pred_thresh == 0)
missed_churners = X_test[false_negatives]

print(f"Number of Missed Churners: {missed_churners.shape[0]}")
missed_churners.head()

"""## Saving the Improved Model"""

best_recall_model = rfc_weighted  # or ensemble_model
model_data = {"model": best_recall_model, "features_names": X.columns.tolist()}

with open("improved_churn_model.pkl", "wb") as f:
    pickle.dump(model_data, f)

# Save final model and feature names
model_data = {"model": rfc, "features_names": X.columns.tolist()}
with open("final_model.pkl", "wb") as f:
    pickle.dump(model_data, f)

# Save encoders
with open("encoders.pkl", "wb") as f:
    pickle.dump(encoders, f)

print(feature_names)

encoders

"""# Insights & Observations

- Recall jumped from 63% to 78% with threshold lowered to 0.35.

- False Negatives dropped from 139 → 82.

- This means it now detects 78% of churners, allowing early business action.

Precision dropped (52%)— but this is acceptable if the goal is to catch more churners, even at the cost of false alarms.

## Final Recommendation

- Use threshold-tuned model (0.35) in retention campaigns where catching churners is critical.

- Use class-weighted model as your default production model if you want better overall balance.

- Optionally deploy the ensemble for robustness testing and future experimentation.

# Summary

 After experimenting with threshold tuning, class weighting, and ensemble models:

- The threshold-tuned model (0.35) achieved the highest recall (78%), reducing false negatives from 139 to 82, effectively identifying more at-risk customers.

- The class-weighted random forest provided a more balanced trade-off with 79% accuracy and 61% recall, suitable for general-purpose deployment.

- The ensemble model showed stability with a similar performance to class weighting, but no significant recall advantage.

- Depending on business goals, the threshold-tuned model is recommended when maximizing churn capture is a priority.
"""